{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba44e323",
   "metadata": {},
   "source": [
    "\n",
    "# Speech Interface System\n",
    "\n",
    "This portfolio project is a **TTS** and **STT** systems Which a fantastic idea. Below is a step-by-step guide to design and implement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Title: Speech Interface System**\n",
    "**Objective:**  \n",
    "Build a system that integrates speech-to-text (STT) and text-to-speech (TTS) functionalities to create a conversational or task-driven application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343756c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Project Overview**\n",
    "The system will:  \n",
    "1. Convert spoken input into text (STT).  \n",
    "2. Perform a task based on the recognized text (e.g., answering questions, controlling devices, or providing information).  \n",
    "3. Generate natural-sounding speech output in response to user queries (TTS).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32963962",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Technologies and Tools**\n",
    "1. **Programming Language:** Python  \n",
    "2. **STT Frameworks/Tools:**  \n",
    "   - Hugging Face Wav2Vec 2.0  \n",
    "   - Google Cloud Speech-to-Text API  \n",
    "3. **TTS Frameworks/Tools:**  \n",
    "   - NVIDIA Tacotron 2 and WaveGlow  \n",
    "   - Google Cloud Text-to-Speech API  \n",
    "4. **Libraries:**  \n",
    "   - PyTorch  \n",
    "   - TensorFlow  \n",
    "   - Hugging Face Transformers  \n",
    "   - Librosa for audio processing  \n",
    "5. **Deployment Platforms:**  \n",
    "   - Flask or FastAPI for the backend  \n",
    "   - Streamlit for a simple UI  \n",
    "   - Docker for containerization  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f991039",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Project Features**\n",
    "1. **Speech-to-Text Integration:**  \n",
    "   Accept user audio input and transcribe it into text.  \n",
    "\n",
    "2. **Task Processing:**  \n",
    "   Add functionality to process text input. Examples:  \n",
    "   - Answer FAQs using a pre-trained transformer model (e.g., GPT or BERT).  \n",
    "   - Trigger simple actions (e.g., \"Turn on the light\").  \n",
    "\n",
    "3. **Text-to-Speech Integration:**  \n",
    "   Generate natural-sounding audio responses to the processed text.  \n",
    "\n",
    "4. **User Interface:**  \n",
    "   A simple web-based UI where users can record their speech, see the text transcription, and listen to the system's response.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fcdc9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Implementation Steps**\n",
    "#### 1. **Environment Setup**\n",
    "- Install required libraries:  \n",
    "```bash\n",
    "pip install torch transformers librosa flask streamlit google-cloud-speech google-cloud-texttospeech\n",
    "```\n",
    "\n",
    "- Set up Google Cloud credentials if using their APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84281df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    audio, rate = librosa.load(audio_file, sr=16000)\n",
    "    input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=True).input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(predicted_ids)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03b0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def process_text(input_text):\n",
    "    qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "    response = qa_model(question=input_text, context=\"This is a demo context for task processing.\")\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ad7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from tacotron2.text import text_to_sequence\n",
    "\n",
    "def generate_audio(text, output_file=\"output.wav\"):\n",
    "    tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "    waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "    sequence = torch.from_numpy(sequence).long().cuda()\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.infer(sequence)\n",
    "    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666).cpu().numpy()\n",
    "    write(output_file, 22050, (audio * 32767).astype(\"int16\"))\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31d389",
   "metadata": {},
   "source": [
    "### **Speech-to-Text (STT) with Pre-trained Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ad933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load pre-trained Wav2Vec 2.0 model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load and preprocess audio file\n",
    "audio_file = \"audio_sample.wav\"\n",
    "audio, rate = librosa.load(audio_file, sr=16000)  # Ensure 16kHz sampling rate\n",
    "input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=True).input_values\n",
    "\n",
    "# Perform speech-to-text\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Decode predictions\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)[0]\n",
    "print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102109f",
   "metadata": {},
   "source": [
    "### **Text-to-Speech (TTS) with Tacotron 2 and WaveGlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from tacotron2.text import text_to_sequence\n",
    "from tacotron2.model import Tacotron2\n",
    "from waveglow.denoiser import Denoiser\n",
    "\n",
    "# Load pre-trained Tacotron 2 and WaveGlow models\n",
    "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "\n",
    "# Prepare text for TTS\n",
    "text = \"Hello, welcome to text-to-speech synthesis using Tacotron 2 and WaveGlow.\"\n",
    "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.from_numpy(sequence).long().cuda()\n",
    "\n",
    "# Generate mel spectrogram with Tacotron 2\n",
    "mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.infer(sequence)\n",
    "\n",
    "# Generate audio with WaveGlow\n",
    "audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
    "denoiser = Denoiser(waveglow)\n",
    "audio = denoiser(audio, strength=0.01).squeeze().cpu().numpy()\n",
    "\n",
    "# Save the audio file\n",
    "write(\"output.wav\", 22050, (audio * 32767).astype(\"int16\"))\n",
    "print(\"Audio generated: output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b3d50",
   "metadata": {},
   "source": [
    "### **Text-to-Speech (TTS) with Tacotron 2 and WaveGlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from tacotron2.text import text_to_sequence\n",
    "from tacotron2.model import Tacotron2\n",
    "from waveglow.denoiser import Denoiser\n",
    "\n",
    "# Load pre-trained Tacotron 2 and WaveGlow models\n",
    "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "\n",
    "# Prepare text for TTS\n",
    "text = \"Hello, welcome to text-to-speech synthesis using Tacotron 2 and WaveGlow.\"\n",
    "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "sequence = torch.from_numpy(sequence).long().cuda()\n",
    "\n",
    "# Generate mel spectrogram with Tacotron 2\n",
    "mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.infer(sequence)\n",
    "\n",
    "# Generate audio with WaveGlow\n",
    "audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
    "denoiser = Denoiser(waveglow)\n",
    "audio = denoiser(audio, strength=0.01).squeeze().cpu().numpy()\n",
    "\n",
    "# Save the audio file\n",
    "write(\"output.wav\", 22050, (audio * 32767).astype(\"int16\"))\n",
    "print(\"Audio generated: output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a4a1e",
   "metadata": {},
   "source": [
    "### **Using Cloud APIs for STT and TTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02839bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech\n",
    "\n",
    "# Initialize the client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# Load audio file\n",
    "audio_file = \"audio_sample.wav\"\n",
    "with open(audio_file, \"rb\") as f:\n",
    "    audio_content = f.read()\n",
    "\n",
    "# Configure request\n",
    "audio = speech.RecognitionAudio(content=audio_content)\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"en-US\"\n",
    ")\n",
    "\n",
    "# Perform transcription\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "for result in response.results:\n",
    "    print(\"Transcript:\", result.alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19957bc",
   "metadata": {},
   "source": [
    "### **Google Cloud Text-to-Speech:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "# Initialize the client\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Configure the request\n",
    "synthesis_input = texttospeech.SynthesisInput(text=\"Hello, this is a text-to-speech example.\")\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    language_code=\"en-US\",\n",
    "    ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
    ")\n",
    "audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "\n",
    "# Perform TTS\n",
    "response = client.synthesize_speech(\n",
    "    input=synthesis_input, voice=voice, audio_config=audio_config\n",
    ")\n",
    "\n",
    "# Save the output\n",
    "with open(\"output.mp3\", \"wb\") as out:\n",
    "    out.write(response.audio_content)\n",
    "    print(\"Audio content written to file: output.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58091e",
   "metadata": {},
   "source": [
    "### **Custom Model Fine-tuning Workflow**\n",
    "#### **Using Hugging Face for Wav2Vec 2.0 Fine-Tuning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"common_voice\", \"en\", split=\"train[:1%]\")\n",
    "\n",
    "# Pre-process dataset\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"], return_tensors=\"pt\").input_ids[0]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "# Load model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Define training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a665940",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Showcase Ideas**\n",
    "- Recording a demo video showing the system in action.  \n",
    "- Writing a detailed blog post on your portfolio site explaining the project.  \n",
    "- Highlighting the use of machine learning models and libraries in your resume.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
