{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba44e323",
   "metadata": {},
   "source": [
    "\n",
    "# Speech Interface System\n",
    "\n",
    "This portfolio project is a **TTS** and **STT** systems Which a fantastic idea. Below is a step-by-step guide to design and implement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Title: Speech Interface System**\n",
    "**Objective:**  \n",
    "Build a system that integrates speech-to-text (STT) and text-to-speech (TTS) functionalities to create a conversational or task-driven application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343756c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Project Overview**\n",
    "The system will:  \n",
    "1. Convert spoken input into text (STT).  \n",
    "2. Perform a task based on the recognized text (e.g., answering questions, controlling devices, or providing information).  \n",
    "3. Generate natural-sounding speech output in response to user queries (TTS).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32963962",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Technologies and Tools**\n",
    "1. **Programming Language:** Python  \n",
    "2. **STT Frameworks/Tools:**  \n",
    "   - Hugging Face Wav2Vec 2.0  \n",
    "   - Google Cloud Speech-to-Text API  \n",
    "3. **TTS Frameworks/Tools:**  \n",
    "   - NVIDIA Tacotron 2 and WaveGlow  \n",
    "   - Google Cloud Text-to-Speech API  \n",
    "4. **Libraries:**  \n",
    "   - PyTorch  \n",
    "   - TensorFlow  \n",
    "   - Hugging Face Transformers  \n",
    "   - Librosa for audio processing  \n",
    "5. **Deployment Platforms:**  \n",
    "   - Flask or FastAPI for the backend  \n",
    "   - Streamlit for a simple UI  \n",
    "   - Docker for containerization  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f991039",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Project Features**\n",
    "1. **Speech-to-Text Integration:**  \n",
    "   Accept user audio input and transcribe it into text.  \n",
    "\n",
    "2. **Task Processing:**  \n",
    "   Add functionality to process text input. Examples:  \n",
    "   - Answer FAQs using a pre-trained transformer model (e.g., GPT or BERT).  \n",
    "   - Trigger simple actions (e.g., \"Turn on the light\").  \n",
    "\n",
    "3. **Text-to-Speech Integration:**  \n",
    "   Generate natural-sounding audio responses to the processed text.  \n",
    "\n",
    "4. **User Interface:**  \n",
    "   A simple web-based UI where users can record their speech, see the text transcription, and listen to the system's response.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fcdc9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Implementation Steps**\n",
    "#### 1. **Environment Setup**\n",
    "- Install required libraries:  \n",
    "```bash\n",
    "pip install torch transformers librosa flask streamlit google-cloud-speech google-cloud-texttospeech\n",
    "```\n",
    "\n",
    "- Set up Google Cloud credentials if using their APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84281df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    audio, rate = librosa.load(audio_file, sr=16000)\n",
    "    input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=True).input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(predicted_ids)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03b0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def process_text(input_text):\n",
    "    qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "    response = qa_model(question=input_text, context=\"This is a demo context for task processing.\")\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ad7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from tacotron2.text import text_to_sequence\n",
    "\n",
    "def generate_audio(text, output_file=\"output.wav\"):\n",
    "    tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "    waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "    sequence = torch.from_numpy(sequence).long().cuda()\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = tacotron2.infer(sequence)\n",
    "    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666).cpu().numpy()\n",
    "    write(output_file, 22050, (audio * 32767).astype(\"int16\"))\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a665940",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Showcase Ideas**\n",
    "- Record a demo video showing the system in action.  \n",
    "- Write a detailed blog post on your portfolio site explaining the project.  \n",
    "- Highlight the use of machine learning models and libraries in your resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecee297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
